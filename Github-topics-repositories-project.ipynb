{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a67fa58",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on Github\n",
    "\n",
    "## TODO:\n",
    "- Intro about web scraping\n",
    "- Intro about Github and the problem statement\n",
    "- Mention the tools I use (Python, requests, BeautifulSoup, Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015fcc4",
   "metadata": {},
   "source": [
    "## Here are the steps we'll follow:\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\\\n",
    "Repo Name,Username,Stars,Repo URL \\\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js \\\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97468dc7",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Githubï¼š\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300e89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c280c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    # Download page\n",
    "    base_url = 'https://github.com/topics'\n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(base_url))\n",
    "    \n",
    "    # Parse using Beautiful Soup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Get info\n",
    "    topic_dict = {'title':get_topic_titles_url(doc, base_url)[0],\n",
    "              'descriptions':get_topic_desc(doc),\n",
    "              'url':get_topic_titles_url(doc, base_url)[1]}\n",
    "    \n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9f878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles_url(doc, base_url): \n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class':selection_class})\n",
    "    \n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    \n",
    "    topic_urls = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_urls.append(base_url + tag.parent['href'])\n",
    "    \n",
    "    return topic_titles, topic_urls\n",
    "\n",
    "def get_topic_desc(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p',{'class':desc_selector})\n",
    "    \n",
    "    topic_descriptions = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descriptions.append(tag.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422d203",
   "metadata": {},
   "source": [
    "## Get the top 25 repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527496fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_doc(topic_url):\n",
    "    # Download page \n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add8d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(star_tag):\n",
    "    stars_str = star_tag.text.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "def get_repo_info(repo_tag, star_tag):\n",
    "    # return all information about a repo\n",
    "    a_tags = repo_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag)\n",
    "    \n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "def get_topic_repos(topic_doc):  \n",
    "    # Get the h3 tags containing repo title and username\n",
    "    h3_selection = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class':h3_selection})\n",
    "    \n",
    "    # Get star tags\n",
    "    start_selection = 'Counter js-social-count'\n",
    "    star_tags = topic_doc.find_all('span', {'class':a_selection})\n",
    "    \n",
    "    # Get repo info\n",
    "    topic_repos_dict = {\n",
    "    'username':[],\n",
    "    'repo_name':[],\n",
    "    'stars':[],\n",
    "    'repo_url':[]\n",
    "    }\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b79914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(path, topic_url):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_doc(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69992e72",
   "metadata": {},
   "source": [
    "## Put all functions above in the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6a20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repo():\n",
    "    print('Scraping top topic repo from github')\n",
    "    topics_df = scrape_topics()\n",
    "    print('------------------------------------')\n",
    "    \n",
    "    os.makedirs('repo_data', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repo for {}...'.format(row['title']))\n",
    "        scrape_topic('repo_data/' + row['title'] + '.csv', row['url'])\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e1036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top topic repo from github\n",
      "------------------------------------\n",
      "Scraping top repo for 3D...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Ajax...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Algorithm...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Amp...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Android...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Angular...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Ansible...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for API...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Arduino...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for ASP.NET...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Atom...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Awesome Lists...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Amazon Web Services...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Azure...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Babel...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Bash...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Bitcoin...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Bootstrap...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Bot...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for C...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Chrome...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Chrome extension...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Command line interface...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Clojure...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Code quality...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Code review...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Compiler...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for Continuous integration...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for COVID-19...\n",
      "The file already exists. Skipping...\n",
      "Scraping top repo for C++...\n",
      "The file already exists. Skipping...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068c903",
   "metadata": {},
   "source": [
    "## References:\n",
    "https://www.youtube.com/watch?v=RKsLLG-bzEY\n",
    "\n",
    "## Summary:\n",
    "- This is a beginner-freindly web scraping project\n",
    "- I don't scrape all topics and all repos on github, I just scraped all information in one page\n",
    "\n",
    "## Future work:\n",
    "- Try to scrape information on multiple pages\n",
    "- Try to scrape websites that need user logging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
